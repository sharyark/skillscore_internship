{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is generated by chatgpt for imbalanced dataset generation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Output directory\n",
    "output_dir = 'shapes_dataset'\n",
    "classes = ['circle', 'square', 'triangle']\n",
    "distribution = {'circle': 1000, 'square': 1000, 'triangle': 200}  # imbalance\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def draw_shape(shape):\n",
    "    img = np.ones((image_size, image_size, 3), dtype=np.uint8) * 255\n",
    "    color = (0, 0, 0)\n",
    "\n",
    "    if shape == 'circle':\n",
    "        center = (random.randint(20, 44), random.randint(20, 44))\n",
    "        radius = random.randint(10, 18)\n",
    "        cv2.circle(img, center, radius, color, -1)\n",
    "\n",
    "    elif shape == 'square':\n",
    "        top_left = (random.randint(10, 30), random.randint(10, 30))\n",
    "        size = random.randint(20, 30)\n",
    "        bottom_right = (top_left[0] + size, top_left[1] + size)\n",
    "        cv2.rectangle(img, top_left, bottom_right, color, -1)\n",
    "\n",
    "    elif shape == 'triangle':\n",
    "        pt1 = (random.randint(10, 54), random.randint(10, 54))\n",
    "        pt2 = (pt1[0] + random.randint(-20, 20), pt1[1] + random.randint(20, 30))\n",
    "        pt3 = (pt1[0] + random.randint(20, 30), pt1[1] + random.randint(-10, 10))\n",
    "        pts = np.array([pt1, pt2, pt3], np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        cv2.fillPoly(img, [pts], color)\n",
    "\n",
    "    return img\n",
    "\n",
    "for shape in classes:\n",
    "    shape_dir = os.path.join(output_dir, shape)\n",
    "    os.makedirs(shape_dir, exist_ok=True)\n",
    "    for i in range(distribution[shape]):\n",
    "        img = draw_shape(shape)\n",
    "        cv2.imwrite(os.path.join(shape_dir, f'{shape}_{i}.png'), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe8680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a07cb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30104268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "nz = 100  # Noise vector size\n",
    "ngf = 64  # Generator feature map size\n",
    "ndf = 64  # Discriminator feature map size\n",
    "num_epochs = 100\n",
    "lr = 0.002\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d1cc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = \"shapes_dataset/triangle\"\n",
    "output_path = \"generated_triangles\"\n",
    "os.makedirs(output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00520871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26efb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Loader\n",
    "dataset = ImageFolder(root=\"shapes_dataset\", transform=transform)\n",
    "triangle_dataset = [(x, y) for x, y in dataset if dataset.classes[y] == \"triangle\"]\n",
    "triangle_loader = DataLoader(triangle_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80fba021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "985a5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),      # (64x64) -> (32x32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False), # (32x32) -> (16x16)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False), # (16x16) -> (8x8)\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False), # (8x8) -> (4x4)\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),       # (4x4) -> (1x1)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)       # [batch_size, 1, 1, 1]\n",
    "        return out.view(-1)     # [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6473e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30a62573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fd73dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed noise for sample generation\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "[0/100] [0/4] Loss_D: 0.1461 Loss_G: 4.1360\n",
      "[1/100] [0/4] Loss_D: 0.3735 Loss_G: 8.7362\n",
      "[2/100] [0/4] Loss_D: 0.1692 Loss_G: 4.8289\n",
      "[3/100] [0/4] Loss_D: 1.6713 Loss_G: 2.5475\n",
      "[4/100] [0/4] Loss_D: 0.0339 Loss_G: 5.6038\n",
      "[5/100] [0/4] Loss_D: 1.6036 Loss_G: 2.4022\n",
      "[6/100] [0/4] Loss_D: 0.2120 Loss_G: 7.9103\n",
      "[7/100] [0/4] Loss_D: 2.6689 Loss_G: 16.3701\n",
      "[8/100] [0/4] Loss_D: 0.1643 Loss_G: 6.9605\n",
      "[9/100] [0/4] Loss_D: 0.0545 Loss_G: 5.7760\n",
      "[10/100] [0/4] Loss_D: 0.0385 Loss_G: 6.3179\n",
      "[11/100] [0/4] Loss_D: 0.0320 Loss_G: 5.2424\n",
      "[12/100] [0/4] Loss_D: 0.0266 Loss_G: 6.5838\n",
      "[13/100] [0/4] Loss_D: 0.0086 Loss_G: 6.6885\n",
      "[14/100] [0/4] Loss_D: 0.0860 Loss_G: 8.4630\n",
      "[15/100] [0/4] Loss_D: 0.0087 Loss_G: 7.0740\n",
      "[16/100] [0/4] Loss_D: 0.0070 Loss_G: 7.6404\n",
      "[17/100] [0/4] Loss_D: 0.0796 Loss_G: 4.1758\n",
      "[18/100] [0/4] Loss_D: 4.1056 Loss_G: 3.5858\n",
      "[19/100] [0/4] Loss_D: 1.3472 Loss_G: 2.3188\n",
      "[20/100] [0/4] Loss_D: 1.7326 Loss_G: 0.6613\n",
      "[21/100] [0/4] Loss_D: 0.9212 Loss_G: 2.6708\n",
      "[22/100] [0/4] Loss_D: 1.7057 Loss_G: 2.1563\n",
      "[23/100] [0/4] Loss_D: 3.8501 Loss_G: 1.6450\n",
      "[24/100] [0/4] Loss_D: 2.3689 Loss_G: 2.7290\n",
      "[25/100] [0/4] Loss_D: 2.1635 Loss_G: 0.6242\n",
      "[26/100] [0/4] Loss_D: 1.3673 Loss_G: 2.5153\n",
      "[27/100] [0/4] Loss_D: 1.4706 Loss_G: 2.6467\n",
      "[28/100] [0/4] Loss_D: 1.4096 Loss_G: 1.0729\n",
      "[29/100] [0/4] Loss_D: 2.5168 Loss_G: 0.2325\n",
      "[30/100] [0/4] Loss_D: 3.0360 Loss_G: 2.0035\n",
      "[31/100] [0/4] Loss_D: 3.2401 Loss_G: 1.6627\n",
      "[32/100] [0/4] Loss_D: 1.1343 Loss_G: 1.5869\n",
      "[33/100] [0/4] Loss_D: 1.5290 Loss_G: 1.1979\n",
      "[34/100] [0/4] Loss_D: 1.6455 Loss_G: 0.5357\n",
      "[35/100] [0/4] Loss_D: 1.1124 Loss_G: 1.3244\n",
      "[36/100] [0/4] Loss_D: 0.9033 Loss_G: 3.1685\n",
      "[37/100] [0/4] Loss_D: 2.2005 Loss_G: 0.7673\n",
      "[38/100] [0/4] Loss_D: 1.2225 Loss_G: 1.0252\n",
      "[39/100] [0/4] Loss_D: 1.0155 Loss_G: 1.5199\n",
      "[40/100] [0/4] Loss_D: 1.1044 Loss_G: 1.7083\n",
      "[41/100] [0/4] Loss_D: 1.4951 Loss_G: 1.2806\n",
      "[42/100] [0/4] Loss_D: 1.1168 Loss_G: 2.3367\n",
      "[43/100] [0/4] Loss_D: 1.4596 Loss_G: 2.2179\n",
      "[44/100] [0/4] Loss_D: 1.5057 Loss_G: 0.8318\n",
      "[45/100] [0/4] Loss_D: 1.6411 Loss_G: 1.5536\n",
      "[46/100] [0/4] Loss_D: 1.1861 Loss_G: 0.7385\n",
      "[47/100] [0/4] Loss_D: 1.2487 Loss_G: 2.0474\n",
      "[48/100] [0/4] Loss_D: 2.6034 Loss_G: 1.6072\n",
      "[49/100] [0/4] Loss_D: 1.0960 Loss_G: 1.3492\n",
      "[50/100] [0/4] Loss_D: 2.4695 Loss_G: 1.5095\n",
      "[51/100] [0/4] Loss_D: 1.1472 Loss_G: 1.3868\n",
      "[52/100] [0/4] Loss_D: 1.4814 Loss_G: 0.2835\n"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "print(\"Training started...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(triangle_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        b_size = real_images.size(0)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        netD.zero_grad()\n",
    "        real_labels = torch.full((b_size,), 1.0, dtype=torch.float, device=device)\n",
    "        fake_labels = torch.full((b_size,), 0.0, dtype=torch.float, device=device)\n",
    "\n",
    "        output_real = netD(real_images)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        output_fake = netD(fake_images.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        lossD = loss_real + loss_fake\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        netG.zero_grad()\n",
    "        # Train G to make D think fake is real\n",
    "        output_fake = netD(fake_images)\n",
    "        lossG = criterion(output_fake, real_labels)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"[{epoch}/{num_epochs}] [{i}/{len(triangle_loader)}] \"\n",
    "                  f\"Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f}\")\n",
    "\n",
    "    # Save sample fake images every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu()\n",
    "            save_image(fake, f\"{output_path}/epoch_{epoch}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67416eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Models and sample images saved.\n"
     ]
    }
   ],
   "source": [
    "# Save final models\n",
    "torch.save(netG.state_dict(), \"generator.pth\")\n",
    "torch.save(netD.state_dict(), \"discriminator.pth\")\n",
    "print(\"Training complete. Models and sample images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef44fc",
   "metadata": {},
   "source": [
    "## now generating part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b33a3aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild the Generator architecture\n",
    "netG = Generator().to(device)\n",
    "netG.load_state_dict(torch.load(\"generator.pth\"))\n",
    "netG.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df4d7cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 synthetic triangle images saved to 'synthetic_triangles/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "os.makedirs(\"synthetic_triangles\", exist_ok=True)\n",
    "\n",
    "n_samples = 500  # Number of images you want to generate\n",
    "batch_size = 64\n",
    "generated = 0\n",
    "\n",
    "while generated < n_samples:\n",
    "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "    with torch.no_grad():\n",
    "        fake_images = netG(noise)\n",
    "\n",
    "    for i in range(fake_images.size(0)):\n",
    "        if generated >= n_samples:\n",
    "            break\n",
    "        img = fake_images[i]\n",
    "        save_image(img, f\"synthetic_triangles/triangle_{generated:04d}.png\", normalize=True)\n",
    "        generated += 1\n",
    "\n",
    "print(f\"{n_samples} synthetic triangle images saved to 'synthetic_triangles/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee14617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4a1ebf-ec76-42e8-8d0b-e2d631cb900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydra/Desktop/genAi/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, default_data_collator\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d157b34e-3ce7-4457-8278-64ba6e812640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39744b4-37b6-4b5b-a655-083857cf483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70386ca3-8862-44a8-822d-e0241d193238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=1536, nx=768)\n",
       "            (q_attn): Conv1D(nf=768, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b29517b-9f22-4fc2-8a39-a2927ad0695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    # output_ids = model.generate(pixel_values, max_length=16, num_beams=4)\n",
    "    output_ids = model.generate(pixel_values, max_length=16)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "# def generate_caption(image_path):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "#     # Set decoder_start_token_id if it's missing\n",
    "#     if model.config.decoder_start_token_id is None:\n",
    "#         model.config.decoder_start_token_id = tokenizer.bos_token_id  # or eos_token_id if your tokenizer doesn't have bos\n",
    "\n",
    "#     # Use greedy decoding or sampling\n",
    "#     output_ids = model.generate(pixel_values, max_length=16, do_sample=False)\n",
    "\n",
    "#     caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "#     return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a705c7d-2d15-4a69-9a59-b2848fa68ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating captions for artifact images...\n",
      "\n",
      "Processing artifact_images/stone_axe.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. You should pass an instance of `Cache` instead, e.g. `past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone_axe.png => a statue of a man with a sword in his hand \n",
      "Processing artifact_images/Hieroglph_wall.jpg\n",
      "Hieroglph_wall.jpg => a series of photos of a variety of animals \n",
      "Processing artifact_images/flint_knife_stone_age.png\n",
      "flint_knife_stone_age.png => a black and white photo of a knife \n",
      "Processing artifact_images/mjollnir-6074194_960_720.jpg\n",
      "mjollnir-6074194_960_720.jpg => a small toy elephant with a bow on it's head \n",
      "Processing artifact_images/Samartian-Persian_necklace_and_amulet.png\n",
      "Samartian-Persian_necklace_and_amulet.png => a large gold and black clock with a face on it \n",
      "Processing artifact_images/69bcabd8-5ad1-42fe-ae74-7032455ecfff.jpg\n",
      "69bcabd8-5ad1-42fe-ae74-7032455ecfff.jpg => a black and white object with a red stripe \n",
      "Processing artifact_images/Birckala_1017_spoon.jpg\n",
      "Birckala_1017_spoon.jpg => a small white object with a black and white pattern \n",
      "Processing artifact_images/museum-7995207_1280.jpg\n",
      "museum-7995207_1280.jpg => a vase with a flower on top of it \n",
      "Processing artifact_images/images.jpeg\n",
      "images.jpeg => a black and white vase with a white face \n",
      "Processing artifact_images/griffin-171407_640.jpg\n",
      "griffin-171407_640.jpg => a statue of a man sitting on top of a table \n",
      "Processing artifact_images/Mycenaean_stirrup_vase_Louvre_AO19201 (1).jpg\n",
      "Mycenaean_stirrup_vase_Louvre_AO19201 (1).jpg => a large ceramic vase with a picture of a horse on it \n",
      "Processing artifact_images/1024px-The_Curmsun_Disc_-_Obverse.png\n",
      "1024px-The_Curmsun_Disc_-_Obverse.png => a plate of food with a bunch of small crumbs on it \n",
      "Processing artifact_images/90f757c9-4b43-4b3d-a0fd-a1471997a7f4.jpg\n",
      "90f757c9-4b43-4b3d-a0fd-a1471997a7f4.jpg => a small vase with a flower in it \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "results = []\n",
    "print(\"Generating captions for artifact images...\\n\")\n",
    "IMAGE_FOLDER = \"artifact_images\"\n",
    "for filename in os.listdir(IMAGE_FOLDER):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        full_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "        print(f\"Processing {full_path}\")\n",
    "        caption = generate_caption(full_path)\n",
    "        results.append((filename, caption))\n",
    "        print(f\"{filename} => {caption}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faac874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8055d07-78dc-4298-b0fc-cf88db125f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Captions saved to 'artifact_captions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "CSV_OUTPUT = \"artifact_captions.csv\"\n",
    "df = pd.DataFrame(results, columns=[\"filename\", \"generated_caption\"])\n",
    "df[\"critique_notes\"] = \"\"  # Leave blank for manual review\n",
    "df.to_csv(CSV_OUTPUT, index=False)\n",
    "\n",
    "print(f\"\\nCaptions saved to '{CSV_OUTPUT}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a5883cc-8a79-4102-8669-db236022176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d485ae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>critique_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stone_axe.png</td>\n",
       "      <td>a statue of a man with a sword in his hand</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hieroglph_wall.jpg</td>\n",
       "      <td>a series of photos of a variety of animals</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flint_knife_stone_age.png</td>\n",
       "      <td>a black and white photo of a knife</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mjollnir-6074194_960_720.jpg</td>\n",
       "      <td>a small toy elephant with a bow on it's head</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samartian-Persian_necklace_and_amulet.png</td>\n",
       "      <td>a large gold and black clock with a face on it</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69bcabd8-5ad1-42fe-ae74-7032455ecfff.jpg</td>\n",
       "      <td>a black and white object with a red stripe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Birckala_1017_spoon.jpg</td>\n",
       "      <td>a small white object with a black and white pa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>museum-7995207_1280.jpg</td>\n",
       "      <td>a vase with a flower on top of it</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>images.jpeg</td>\n",
       "      <td>a black and white vase with a white face</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>griffin-171407_640.jpg</td>\n",
       "      <td>a statue of a man sitting on top of a table</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename  \\\n",
       "0                              stone_axe.png   \n",
       "1                         Hieroglph_wall.jpg   \n",
       "2                  flint_knife_stone_age.png   \n",
       "3               mjollnir-6074194_960_720.jpg   \n",
       "4  Samartian-Persian_necklace_and_amulet.png   \n",
       "5   69bcabd8-5ad1-42fe-ae74-7032455ecfff.jpg   \n",
       "6                    Birckala_1017_spoon.jpg   \n",
       "7                    museum-7995207_1280.jpg   \n",
       "8                                images.jpeg   \n",
       "9                     griffin-171407_640.jpg   \n",
       "\n",
       "                                   generated_caption  critique_notes  \n",
       "0        a statue of a man with a sword in his hand              NaN  \n",
       "1        a series of photos of a variety of animals              NaN  \n",
       "2                a black and white photo of a knife              NaN  \n",
       "3      a small toy elephant with a bow on it's head              NaN  \n",
       "4    a large gold and black clock with a face on it              NaN  \n",
       "5        a black and white object with a red stripe              NaN  \n",
       "6  a small white object with a black and white pa...             NaN  \n",
       "7                 a vase with a flower on top of it              NaN  \n",
       "8          a black and white vase with a white face              NaN  \n",
       "9       a statue of a man sitting on top of a table              NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)  # Display the first 10 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6411c",
   "metadata": {},
   "source": [
    "## fine tune on manual corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "612ff65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcddd27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>critique_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stone_axe.png</td>\n",
       "      <td>an ancient stone axe tool used in the stone age</td>\n",
       "      <td>it's an axe from the stone age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hieroglph_wall.jpg</td>\n",
       "      <td>ancient Egyptian hieroglyphs carved into a sto...</td>\n",
       "      <td>Egyptian Hieroglyph wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flint_knife_stone_age.png</td>\n",
       "      <td>a primitive flint knife used in prehistoric times</td>\n",
       "      <td>stone age flint knife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mjollnir-6074194_960_720.jpg</td>\n",
       "      <td>a Norse artifact shaped like Thor's hammer</td>\n",
       "      <td>it's Thor's hammer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samartian-Persian_necklace_and_amulet.png</td>\n",
       "      <td>a Persian necklace and amulet with intricate d...</td>\n",
       "      <td>Persian necklace and amulet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename  \\\n",
       "0                              stone_axe.png   \n",
       "1                         Hieroglph_wall.jpg   \n",
       "2                  flint_knife_stone_age.png   \n",
       "3               mjollnir-6074194_960_720.jpg   \n",
       "4  Samartian-Persian_necklace_and_amulet.png   \n",
       "\n",
       "                                   generated_caption  \\\n",
       "0    an ancient stone axe tool used in the stone age   \n",
       "1  ancient Egyptian hieroglyphs carved into a sto...   \n",
       "2  a primitive flint knife used in prehistoric times   \n",
       "3         a Norse artifact shaped like Thor's hammer   \n",
       "4  a Persian necklace and amulet with intricate d...   \n",
       "\n",
       "                   critique_notes  \n",
       "0  it's an axe from the stone age  \n",
       "1        Egyptian Hieroglyph wall  \n",
       "2           stone age flint knife  \n",
       "3              it's Thor's hammer  \n",
       "4     Persian necklace and amulet  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"manually_observed.csv\")\n",
    "df = df.dropna(subset=[\"filename\", \"critique_notes\"])\n",
    "df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "df[\"critique_notes\"] = df[\"critique_notes\"].astype(str).str.strip()\n",
    "df = df[df[\"critique_notes\"] != \"\"]  # Remove empty captions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4f9d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor, tokenizer, max_length=64):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row[\"filename\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        caption = row[\"critique_notes\"]\n",
    "        if not isinstance(caption, str):\n",
    "            caption = str(caption)\n",
    "        caption = caption.strip() or \"an artifact\"\n",
    "\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in loss\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c660713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Enable decoder start/pad token\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Set generation length\n",
    "model.config.max_length = 64\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d15d1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydra/Desktop/genAi/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"./artifact_images\"  # Directory containing artifact images\n",
    "dataset = ArtifactDataset(df, image_dir, processor, tokenizer)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./vit_gpt2_finetuned_artifacts\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b20d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4371/1030909095.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bde433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydra/Desktop/genAi/lib/python3.12/site-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=5.550984191894531, metrics={'train_runtime': 64.5794, 'train_samples_per_second': 1.007, 'train_steps_per_second': 0.31, 'total_flos': 1.173015292280832e+16, 'train_loss': 5.550984191894531, 'epoch': 5.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0a20c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vit_gpt2_finetuned_artifacts_2/preprocessor_config.json']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./vit_gpt2_finetuned_artifacts_2\")\n",
    "tokenizer.save_pretrained(\"./vit_gpt2_finetuned_artifacts_2\")\n",
    "processor.save_pretrained(\"./vit_gpt2_finetuned_artifacts_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d1d0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local path\n",
    "model_path = \"./vit_gpt2_finetuned_artifacts_2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "processor = ViTImageProcessor.from_pretrained(model_path, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8721890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "\n",
    "# Load the model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "    pixel_values,\n",
    "    max_length=32,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.5,\n",
    "    no_repeat_ngram_size=3,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id\n",
    ")\n",
    "\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e80eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a wooden table topped with a circle shaped food item  edited from scrapbooks, including several pieces of animal figurines and plates filled to the brim , as\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/hydra/Desktop/Skill_score/skillscore_internship/task2/artifact_images/1024px-The_Curmsun_Disc_-_Obverse.png\"  # Replace with your image file path\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d62e07b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing artifact_images/stone_axe.png\n",
      "a carved headstone with a lion figure on it young in the saddle wearing a cowboy hat and necktie while kneeling over top of it next to some\n",
      "Processing artifact_images/Hieroglph_wall.jpg\n",
      "a wall of photos showing different colored tiles \n",
      "very much like an english cartoon character with black and white stripes in front, a monkey holding two scissors next\n",
      "Processing artifact_images/flint_knife_stone_age.png\n",
      "a black knife stuck in a bone and orange string guy\n",
      "Processing artifact_images/mjollnir-6074194_960_720.jpg\n",
      "a rusted gold guitar sitting on top of a table large sculpture in the middle of the road, with large pieces of glass around it and metal sp\n",
      "Processing artifact_images/Samartian-Persian_necklace_and_amulet.png\n",
      "a fake skull on the back of a antique urn in front oe clock face with white background and an arrow pointing down to be fifty five twenty six\n",
      "Processing artifact_images/69bcabd8-5ad1-42fe-ae74-7032455ecfff.jpg\n",
      "a bright orange object attached to the outside surface of a street large and red light shining from behind, with very little visible damage as it moves through the\n",
      "Processing artifact_images/Birckala_1017_spoon.jpg\n",
      "a metal object that has a tiny dot on top small image with black background and pattern of outline, near some brush in the foreground; small details visible\n",
      "Processing artifact_images/museum-7995207_1280.jpg\n",
      "a wooden pot sitting on top of a table black and white photo with candles next to it in sunlight, surrounded by other pots that appear as if they\n",
      "Processing artifact_images/images.jpeg\n",
      "a ceramic vase sitting on a table large sized statue of a cow standing up in the air near another plate with it's foot pointed towards a camera\n",
      "Processing artifact_images/griffin-171407_640.jpg\n",
      "a sculpture of a fish with eyes painted on large ivory heads at the end, near the center _____________________\n",
      "Processing artifact_images/Mycenaean_stirrup_vase_Louvre_AO19201 (1).jpg\n",
      "a unique looking decorative ceramic vase green color and design with a black handle on the lid, next to a metal sculpture for decoration or embellishment of\n",
      "Processing artifact_images/1024px-The_Curmsun_Disc_-_Obverse.png\n",
      "a white plate topped with pieces of food ____________________\n",
      "Processing artifact_images/90f757c9-4b43-4b3d-a0fd-a1471997a7f4.jpg\n",
      "a small wooden and vase sitting in the middle of a room large white pottery decoration on counter top with other colored items behind it, also displaying\n"
     ]
    }
   ],
   "source": [
    "IMAGE_FOLDER = \"artifact_images\"\n",
    "for filename in os.listdir(IMAGE_FOLDER):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        full_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "        print(f\"Processing {full_path}\")\n",
    "        caption = generate_caption(full_path)\n",
    "        # break\n",
    "        print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f3093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
